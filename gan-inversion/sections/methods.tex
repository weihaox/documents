\section{GAN Inversion Methods}
\label{sec:model}



\subsection{Inversion Methods}
\label{sec:techniques}
There are primarily three main techniques of GAN inversion, \ie, projecting an image onto the latent space based on learning, optimization, and hybrid formulations, as shown in Figure~\ref{fig:inversion_types}.
The learned inverse representations also have other characteristics, \ie, interpretable directions, semantic-aware, layer-wise, non-interference, region-of-interesting, and out-of-distribution. 
Table~\ref{tab:taxonomy} lists characteristics of existing state-of-the-art GAN inversion methods.

\figtype

\subsubsection{Learning-based GAN Inversion}
\label{sec:learning-based}
Learning-based GAN inversion~\cite{perarnau2016invertible,zhu2016generative,bau2019inverting} typically involves training an encoding neural network $E(x; \theta_E)$ to map an image $x$ to the latent code $\z$ by

\begin{equation}
\theta_E^* = \underset{\theta_E}{\arg\min} \sum_{n} \mathcal{L} (G(E(x_n; \theta_E)), \,x_n),
\label{eqn:rec_train}
\end{equation}
where $x_n$ denotes the $n$-th image in the dataset.
The architecture of the predictive model $E$ is usually equivalent to that of the discriminator $D$ of the adversarial network, and only varies in the final layer.
The objective in~\eqref{eqn:rec_train} is reminiscent of an auto-encoder pipeline, with an encoder $E$ and a decoder $G$. The decoder $G$ is fixed throughout the training.
While the optimization problem described in~\eqref{eqn:def} is the same as the learning objective~\eqref{eqn:rec_train}, the learning-based approach often achieves better performance than direct optimization and does not fall into local optima~\cite{zhu2016generative,aberdam2020invert}.

For example, Perarnau~\etal~\cite{perarnau2016invertible} propose the Invertible Conditional GAN (ICGAN) method in which an image $x$ is represented by a latent representation $\z$ and an attribute vector $y$, and a modified image $x^{\prime}$ can be generated by changing $y$.
This approach consists of training an encoder $E$ with a trained CGAN. 
Different from the method by Zhu~\etal~\cite{zhu2016generative}, this encoder $E$ is composed of two sub-encoders: $E_{z}$, which encodes an image to $\z$, and $E_{y}$, which encodes an image to $y$. 
To train $E_{z}$, this method uses the generator to create a dataset of generated images $x^{\prime}$ and their latent vectors $\z$, 
minimizes a squared reconstruction loss $\mathcal{L}_{ez}$ between $\z$ and $E_{z}(G(\z, y^{\prime}))$ and improves $E_{y}$ by directly training with $\|y-E_{y}(x)\|_{2}^{2}$. 
Here, $E_{y}$ is initially trained by using generated images $x^{\prime}$ and their conditional information $y^{\prime}$.
Guan~\etal~\cite{guan2020faster} propose the embedding network, which consists of two encoders, an identity encoder $E_{id}$ to extract identity from the input image $x$, and an attribute Encoder $E_{attr}$ to extract attributes from $x$. 
Given an input image $x$ in $256 \times 256$ resolution, the embedding network generates its latent code $\mathbf{w_e}$, which is then set as the initialization of the iterator.
The output of iterator $\mathbf{w_o}$ in turns supervises the training of the embedding network, using the MSE loss, LPIPS loss and latent code loss.
Tewari~\etal~\cite{tewari2020stylerig} develop an interpretable model over face semantic parameters of a pretrained StyleGAN $\mathcal{S}$.
Given a latent code $\w \in \R^{l}$ that corresponds to an image $I$, and a vector $\p \in \R^f$ of semantic control parameters, this method learns a function $\mathcal{R}$ that outputs a modified latent code ${\w}^{\prime} = \mathcal{R}(\w, \p)$. 
The modified latent code $\w^{\prime}$ is designed to map to a face image $I^{\prime} = \mathcal{S}({\w}^{\prime})$ that obeys the control parameters $\p$.
The encoder $\mathcal{R}$ is trained separately for the different modes of control, \ie, pose, expression and illumination, which is implemented based on a linear two-layer MLP and is trained in a self-supervised manner based on two-way cycle consistency losses and a differentiable face reconstruction network.

To better reuse the layer-wise representations learned by the StyleGAN model, Xu~\etal~\cite{xu2020ghfeat} propose to train a hierarchical encoder by treating the pretrained StyleGAN generator as a learned loss (similar to perceptual loss~\cite{johnson2016perceptual} using learned VGG~\cite{simonyan2014very}). 
The learned disentangled multi-level visual features $\{\y^{(\ell)}\}^L_{\ell=1}$  are then fed into per-layer adaptive instance normalization (AdaIN)~\cite{huang2017adain} of the fixed StyleGAN generator to obtain the desired images by replacing the original style code:
\begin{equation}
\mathtt{AdaIN}({x}_i^{(\ell)}, \y^{(\ell)}) = \y_{s,i}^{(\ell)}\ \frac{{x}_i^{(\ell)} - \mu({x}_i^{(\ell)})}{\sigma({x}_i^{(\ell)})} + \y_{b,i}^{(\ell)},
\label{eq:adain}  
\end{equation}
where $L$ is the number of convolutional layers, ${x}=G(z)$, ${x}_i^{(\ell)}$ indicates the $i$-th channel of the normalized feature map from the $\ell$-th layer, $\mu(\cdot)$ and $\sigma(\cdot)$ denote the mean and variance respectively, $\y_s^{(\ell)}$ and $\y_b^{(\ell)}$ correspond to the scale and weight parameters in AdaIN respectively.

Although some methods\cite{donahue2016adversarial,dumoulin2016adversarially} use additive encoder networks to learn the inverse mapping of GANs, we do not categorize them as GAN inversion since their goals are to \emph{jointly train} the encoder with both the generator and the discriminator, instead of determining the latent space of a trained GAN model.

\subsubsection{Optimization-based GAN Inversion}
\label{sec:optimization-based}

Existing optimization-based GAN inversion methods typically reconstruct a target image by optimizing over the latent vector
\begin{equation}
\z^* = \underset{\z}{\arg\min}\, \ell(x, G(\z; \theta)),
\label{eqn:opt}
\end{equation}
where $x$ is the target image and $G$ is a GAN generator parameterized by $\theta$.
Optimization-based GAN inversion typically optimizes the latent code based on either gradient descent~\cite{creswell2018inverting,lipton2017precise,ma2018invertibility,creswell2018inverting,abdal2019image2stylegan,abdal2020image2stylegan2,lipton2017precise} or other iterative algorithms~\cite{voynov2020latent, ramesh2018spectral}.
For example, Ramesh~\etal~\cite{ramesh2018spectral} and Voynov~\etal~\cite{voynov2020latent} use the Jacobian Decomposition to analyze the latent space of a pretrained GAN model. 
Specifically, the left eigenvectors of the Jacobian matrix for the generator are used to indicate the most disentangled directions. 
In these methods, an interpretable curve is constructed by a latent point $z_0$ and a direction from the corresponding $k$-th left eigenvector. 
Voynov~\etal~\cite{voynov2020latent} show that once the latent vector moves along that curve, the generated image appears to be transformed smoothly. 
Nevertheless, while the constructed curves often capture interpretable transformations, the effects are typically entangled (\ie, lighting and geometrical transformations appear simultaneously). 
This method also requires an expensive (in terms of both memory and runtime) iterative process for computing the Jacobian matrix on each step of the curve construction, and has to be applied in each latent code independently. 
As such, a lightweight approach that identifies a set of directions at once is also developed. 
We note that the method by Voynov~\etal~\cite{voynov2020latent} can be applied to a larger number of directions while the approach by Ramesh~\etal~\cite{ramesh2018spectral} is limited to the maximal number of discovered directions equal to the dimension of latent space.
 
To deal with the local minima issue, numerous optimization methods have been developed. Generally, there are two types of optimizers: gradient-based (ADAM~\cite{kingma2014adam}, L-BFGS~\cite{liu1989limited}, Hamiltonian Monte Carlo (HMC)~\cite{duane1987hybrid}) and gradient-free (Covariance Matrix Adaptation (CMA)~\cite{hansen2001cma}).
For example, the ADAM optimizer is used in the Image2StyleGAN~\cite{abdal2019image2stylegan} and the L-BFGS scheme is used in the approach by Zhu~\etal~\cite{zhu2016generative}.
Huh~\etal~\cite{huh2020transforming} experiment various gradient-free optimization methods in the Nevergrad library~\cite{nevergrad} with the default optimization hyper-parameters. They find that CMA and its variants BasinCMA perform the best for optimizing the latent vector when inverting images in challenging datasets (\eg, LSUN Cars) to the latent space of StyleGAN2~\cite{karras2020analyzing}.

Another important issue for optimization-based GAN inversion is initialization. 
Since~\eqref{eqn:def} is highly non-convex, the reconstruction quality strongly relies on a good initialization of $\z$ (sometimes $\w$ for StyleGAN~\cite{karras2019style}).
The experiments show that using different initializations leads to a significant perceptual difference in generated images~\cite{radford2016dcgan,brock2018large,karras2017progressive,karras2019style}. 
An intuitive solution is to start from several random initializations and obtain the best result with the minimal cost. 
Image2StyleGAN~\cite{abdal2019image2stylegan} analyzes two choices for the initialization $\w^{*}$ based on random selection and mean latent code $\overline{\w}$ motivated by the observation from~\cite{karras2019style} that the distance to $\overline{\w}$ can be used to identify low quality faces. 
However, a prohibitively large number of random initializations may be required to obtain a stable reconstruction~\cite{zhu2016generative}, which makes real-time processing impossible. 
Thus, some~\cite{zhu2016generative,tewari2020stylerig,guan2020faster} instead train a deep neural network to minimize~\eqref{eqn:def} directly as introduced in Section~\ref{sec:learning-based}.

We note that some~\cite{zhu2016generative,bau2019inverting,guan2020faster} propose to use an encoder to provide better initialization for optimization (which will discussed in Section~\ref{sec:hybrid}).

\subsubsection{Hybrid GAN Inversion}
\label{sec:hybrid}

The hybrid methods~\cite{zhu2016generative,bau2019seeing,bau2019inverting,zhu2020indomain,guan2020faster} exploit advantages of both approaches discussed above. 
As one of the pioneering work in this field, Zhu~\etal~\cite{zhu2016generative} propose a framework that first predicts $\z$ of a given real photo $x$ by training a separate encoder $E(x; \theta_E)$, then uses the obtained $\z$ as the initialization for optimization.
The learned predictive model serves as a fast bottom-up initialization for the non-convex optimization problem~\eqref{eqn:def}.

The subsequent studies basically follow this framework and have proposed several variants.
For example, to invert $G$, Bau~\etal~\cite{bau2019inverting} begin with training a network $E$ to obtain a suitable initialization of the latent code $\z_{0}=E(x)$ and its intermediate representation $\rr_{0}=g_{n}(\cdots(g_{1}(\z_{0})))$, where $g_{n}(\cdots(g_{1}(\cdot)))$ in a layer-wise representation of $G(\cdot)$.
This method then uses $\rr_{0}$ to initialize a search for $\rr^{*}$ to obtain a reconstruction $x^{\prime}=G(\rr^{*})$ close to the target $x$ 
(see Section~\ref{sec:layerwise} for more details).
Zhu~\etal~\cite{zhu2020indomain} show that in most existing methods, the generator $G$ does not provide its domain knowledge to guide the training of encoder $E$ since the gradients from $G(\cdot)$ are not taken into account at all. 
As such, a domain-specific GAN inversion approach is developed, which both reconstructs the input image and ensures the inverted code meaningful for semantic editing 
(see Section~\ref{sec:semantic-aware} for more details).

Without using the above framework, Guan~\etal~\cite{guan2020faster} propose a collaborative learning framework for StyleGAN inversion, where the embedding network gives a reasonable latent code initialization $\mathbf{w_e}$ for the optimization-based iterator, and the updated latent code from the iterator $\mathbf{w_o}$, in turn, supervises the embedding network to produce more accurate latent codes. The objective functions of embedding network $\mathcal{L}_{emb}$ and iterator $\mathcal{L}_{opt}$ are 
\begin{equation}
\begin{aligned}
\mathcal{L}_{emb} & = \lambda_1 \underbrace{||\mathbf{w_e} - \mathbf{w_o}||_2^2}_{\text{latent loss}} + \lambda_2 \underbrace{||{x_e} - {x_o}||_2^2}_{\text{image loss}} + \lambda_3 \underbrace{\Phi({x_e}, {x_o})}_{\text{feature loss}}, \\
\mathcal{L}_{opt} & = ||G(\w) - {x}||_2^2 + \alpha \Phi(G(\w), x),\\
\end{aligned}
\end{equation}
where $\w \in \mathcal{W^+}$ is the latent code to be optimized, $G$ is a frozen generator of StyleGAN pretrained on the FFHQ dataset~\cite{karras2019style}, ${x_e} = G(\mathbf{w_e})$ and ${x_o} = G(\mathbf{w_o})$ are generated from $\mathbf{w_e}$ and $\mathbf{w_o}$ by the StyleGAN generator $G$, $\Phi(\cdot)$ is the LPIPS loss~\cite{zhang2018unreasonable}, and $\lambda_1$, $\lambda_2$, $\lambda_3$, $\alpha$ are the loss weights.




\subsubsection{Closed-Form Solution}
\label{sec:closed}
Very recently, two methods~\cite{nurit2020steerability,shen2020closedform} found that the interpretable directions can be directly computed in \textit{closed-form}, without any kinds of training or optimization. 
To be specific, Nurit~\etal~\cite{nurit2020steerability} observe that the output of the first layer in BigGAN~\cite{brock2018large} (the first layer maps $\z$ into a tensor with low spatial resolution) already has spatial coordinates and determines the coarse structure of the generated image, which suggests that applying the geometric transformation to the output of the first layer is similar to applying it directly to the generated image, \ie, $G(\z+\q)\approx \mathcal{T}\{G(\z)\}$ for every $\z$. 
$G$ is a pretrained generator, $\mathcal{T}$ is the desired transformation in the image, and $\q$ is the target direction in the latent space.
The goal is to bring $\W(\z+\q)+\bb$ as close as possible to $\PP(\W\z+\bb)$.
$\PP$ denotes the matrix corresponding to $\mathcal{T}$ in the resolution of the first layer's output. 
$\W$ and $\bb$ are the weights and biases of the first layer.
To guarantee that this holds over random draws of $\z$, they formulate the problem as
\begin{equation}
\min_{\q}\;
\E_{\z \sim p_{\z}}
[\|\D\Big(\W(\z+\q)+\bb-\PP(\W\z+\bb)\Big)\|^2],
\label{eqn:LinObjNurit}
\end{equation}
where $p_{\z}$ is the probability density function of $\z$ and $\D$ is a diagonal matrix that can be used to assign different weights to different elements of the tensors. 
Assuming $\E[\z]=0$, a closed-form solution $\q$ can be obtained for the optimal linear direction corresponding to transformation $\PP$,
\begin{equation}
\q = (\W^T\!\D^2\,\W)^{-1}\W^T\!\D^2(\PP-\I)\,\bb.
\label{eqn:q}
\end{equation}
With the linear trajectories $\z+\q$, the generated image inevitably becomes distorted or even meaningless after many steps.
Thus, they further propose nonlinear trajectories to remedy the problems. 
The walks in the latent space have the form $\z_{n+1}=\M\z_{n}+\q$, where the transformation $\PP$ is determined by a vector $\q$ and a diagonal matrix $\M$.
Problem~\eqref{eqn:LinObjNurit} is then formulated as
\begin{equation}
\min_{\M,\q}\;\E_{\z \sim p_{\z}}[\|\D\Big(\W(\M\z+\q)+\bb-\PP(\W\z+\bb)\Big)\|^2].
\label{eq:LinNonObjNurit}
\end{equation}
Assuming again that $\E[\z]=0$ and making an additional assumption that $\E[\z\z^T]=\sigma^2_z \I$, the solution for $\q^*$ remains the same as in \eqref{eqn:q} and the solution for $\M$ is
\begin{equation}
\M_{i,i} = \frac{\w_i^T\D^2\PP\,\w_i}{\w_i^T\D^2\,\w_i},
\label{eqn:M}
\end{equation}
where $\w_i$ is the $i$-th column of $\W$.

Shen~\etal~\cite{shen2020closedform} observe that the semantic transformation of an image, usually denoted by moving the latent code towards a certain direction $\n^{\prime} = \z + \alpha \n$, is actually determined by the latent direction $\n$ and is independent of the sampled code $\z$.
Based on that, they turn into finding the directions $\n$ that can cause a significant change in the output image $\Delta\y$, \ie, $\Delta\y =\y^{\prime}-\y= (\A(\z + \alpha\n) + \bb) - (\A\z + \bb) = \alpha\A\n$, where $\A$ and $\bb$ are the weight and bias of certain layers in $G$, respectively. 
The obtained formula, $\Delta\y = \alpha\A\n$, suggests that the desired editing with direction $\n$ can be achieved by adding the term $\alpha\A\n$ onto the projected code and indicates that the weight parameter $\A$ should contain the essential knowledge of image variations.
The problem of exploring the latent semantics can thus be factorized by solving the following optimization problem:
\begin{equation}
  \n^* = \underset{\{\n\in\R^d:\ \n^T\n = 1\}}{\arg\max} ||\A\n||_2^2.  
  \label{eq:single-optimization}    
\end{equation}
The desired directions $\n^*$, \ie, a closed-form factorization of latent semantics in GANs, should be the eigenvectors of the matrix $\A^T\A$.

\subsection{Characteristics of GAN Inversion Methods}
\label{sec:characteristics}

We discuss some important characteristics of GAN inversion methods in this section. 

\subsubsection{Interpretable Directions}
\label{sec:interpretable-directions}

Some GAN inversion methods support discovering interpretable directions in the latent space, \ie, controlling the generation process by varying the latent codes $\z$ in the desired directions $\n$ with step $\alpha$, which can often be represented as the vector arithmetic $\z^{\prime}=\z+\alpha\n$.
Such directions are currently discovered in supervised, unsupervised, or self-supervised manners.

\noindent\textbf{Supervised-Setting.} 
Existing supervised learning-based approaches typically randomly sample a large amount of latent codes, synthesize a collection of images, and annotate them with some pre-defined labels by  introducing a pretrained classifier (\eg, predicting face attributes or light directions)~\cite{goetschalckx2019ganalyze,shen2020interpreting,abdal2020styleflow,jahanian2020steerability} or extracting statistical image information (\eg, color variations)~\cite{plumerault2020control}.
For example, to interpret the face representation learned by GANs, Shen~\etal~\cite{shen2020interpreting} employ some off-the-shelf classifiers to learn a hyperplane in the latent space serving as the separation boundary and predict semantic scores for synthesized images.
Abdal~\etal~\cite{abdal2020styleflow} learn a semantic mapping between the $\mathcal{Z}$ space and the $\mathcal{W}$ space using  Continuous Normalizing Flows (CNF).
Both methods rely on the availability of attributes (typically obtained by a face classifier network), which might be difficult to obtain for new datasets and could require a manual labeling effort.
Jahanian~\etal~\cite{jahanian2020steerability} optimize trajectories (both linear and non-linear, as shown in Figure~\ref{fig:walk}) 
in a self-supervised manner. 
Taking the linear walk $\ww$ for example, given an inverted source image $G(\z)$, they learn $\ww$ by minimizing the objective function 
\begin{equation}
\w^* = \underset{\w}{\arg\min} {\E}_{\z,\alpha} [\mathcal{L} ( G(\z\!+\!\alpha \w), \texttt{edit}(G(\z), \alpha))].
\label{eq:optimal_w}
\end{equation}
Here, $\mathcal{L}$ measures the distance between the generated image $G(\z+\alpha \ww)$ after taking an $\alpha$-step in the latent direction and the target \texttt{edit}($G(\z), \alpha$) derived from the source image $G(\z)$.

\figwalk

\noindent\textbf{Unsupervised-Setting.} 
The supervised setting would introduce bias into the experiment since the sampled codes and synthesized images used as supervision are different in each sampling and may lead to different discoveries of interpretable directions~\cite{shen2020closedform}. 
It also severely restricts a range of directions that existing approaches can discover, especially when the labels are missing. 
Furthermore, the individual controls discovered by these methods are typically entangled, affecting multiple attributes, and are often non-local.
Thus, some~\cite{voynov2020latent,lu2020discovery,eric2020GANSpace,cherepkov2020navigating} aim to discover interpretable directions in the latent space in an unsupervised manner, \ie, without the requirement of paired data.
For example, Härkönen~\etal~\cite{eric2020GANSpace} create interpretable controls for image synthesis by identifying important latent directions based on PCA applied in the latent or feature space. The obtained principal components correspond to certain attributes and selective application of the principal components allows control of features.
Shen~\etal~\cite{shen2020closedform} and Nurit~\etal~\cite{nurit2020steerability} propose to directly compute the interpretable directions in closed form from the pretrained models, without any kinds of training or optimization (see Section~\ref{sec:closed} for more details).

\subsubsection{Semantic-Aware}
\label{sec:semantic-aware}

GAN inversion methods with semantic-aware properties can perform image reconstruction at the pixel level and align the inverted code with the knowledge that emerged in the latent space. 
Semantic-aware latent codes can better support image editing by reusing the rich knowledge encoded in the GAN models.
As shown on the upper panel of Figure~\ref{fig:indomain}, existing approaches typically sample a collection of latent codes $\z$ randomly and feed them into $G(\cdot)$ to get the corresponding synthesis $x^{\prime}$. 
The encoder $E(\cdot)$ is then trained by
\begin{equation}
\min_{\Theta_{E}} \mathcal{L}_{E}=\|\z-E(G(\z))\|_{2},
\end{equation}
where $\|\cdot\|_{2}$ denotes the $l_{2}$ distance and $\Theta_{E}$ represents the parameters of the encoder $E(\cdot)$.

Collins~\etal~\cite{collins2020uncovering} use a latent object representation to synthesize images with different styles and reduce artifacts.
However, the supervision by only reconstructing $\z$ is not sufficient to train an accurate encoder. 
To alleviate this issue, Zhu~\etal~\cite{zhu2020indomain} propose a domain-specific GAN inversion approach to recover the input image at both the pixel and semantic levels.
This method first trains a domain-guided encoder to map the image space to the latent space such that all codes produced by the encoder are in-domain. Then, they perform instance-level domain-regularized optimization by involving the encoder as a regularization term. Such optimization helps to better reconstruct the pixel values without affecting the semantic property of the inverted code.
The training process is formulated as
\begin{equation} 
\begin{aligned}
\min_{\Theta_{E}} \mathcal{L}_{E}=\|x-G(E(x))\|_{2} 
&+\lambda_1\|F(x)-F(G(E(x)))\|_{2} \\ 
&-\lambda_2 {\E}[D(G(E(x)))],
\end{aligned}
\end{equation} 
where $F(\cdot)$ represents the VGG feature extraction, $\E[D(\cdot)]$ is the discriminator loss and $\lambda_1$ and $\lambda_2$ are the perceptual and discriminator loss weights.

The inverted code from the proposed domain-guided encoder can well reconstruct the input image based on the pretrained generator and ensure the code itself to be semantically meaningful. However, the code still needs refinement to better fit the individual target image at the pixel values.
Based on the domain-guided encoder, Zhu~\etal design a domain-regularized optimization with two modules:
(i) the output of the domain-guided encoder is used as a starting point to avoid local minimum and also shorten the optimization process; and (ii) a domain-guided encoder
is used to regularize the latent code within the semantic domain of the generator. 
The objective function is
\begin{equation}
\begin{aligned}
\z^{*}=\underset{\z}{\arg \min }\|x-G(\z)\|_{2} &+\lambda_1^{\prime}\|F(x)-F(G(\z))\|_{2} \\
&+\lambda_2^{\prime}\|\z-E(G(\z))\|_{2},
\end{aligned} 
\end{equation}
where $x$ is the target image to invert, and $\lambda_1^{\prime}$ and $\lambda_2^{\prime}$ are the loss weights corresponding to the perceptual loss and the encoder regularizer, respectively.

\figindomain

\subsubsection{Layer-Wise}
\label{sec:layerwise}

As it is not feasible to determine the generator for the full inversion problem defined by~\eqref{eqn:def} when the number of layers is large, a few approaches~\cite{bau2019seeing,lei2019inverting,aberdam2020invert} have been developed to solve a tractable sub-problem by decomposing the generator $G$ into layers:
\begin{equation}
G=G_{f}(g_{n}(\cdots((g_{1}(\z)))),
\end{equation}
where $g_{1}, \ldots, g_{n}$ are early layers of $G$, and $G_{f}$ constructs all the later layers of $G$.

The simplest layer-wise GAN inversion is based on one layer. 
We start inverting a single layer to find if $\underset{\z}\min\|x-G(\z) \|_p=0$, a specific formulation of~\eqref{eqn:def}, holds for any $p$-norm. 
Since the problem is non-convex, additional assumptions are required \cite{huang2018provably} for gradient descent to find $\underset{\z}{\arg\min}\|x-G(\z)\|$.
When the problem is realizable, however, to find feasible $\z$ such that $x= \text{ReLU}(\W\z+\bb)$, one could invert the function by solving a linear programming:
\begin{eqnarray}
{\w_i}^\top \z + b_i = x_i, &\forall i \text{ s.t. } x_i>0, \nonumber\\
{\w_i}^\top \z + b_i\leq 0, &\forall i \text{ s.t. } x_i= 0.
\label{eqn:single_layer}
\end{eqnarray}
The solution set of~\eqref{eqn:single_layer} is convex and forms a polytope. 
However, it also possibly includes uncountable feasible points~\cite{lei2019inverting}, which makes it unclear how to invert layer-wisely inversion.
Several approaches make additional assumptions to generalize the above result to deeper neural networks. 
Lei~\etal~\cite{lei2019inverting} assume that the input signal is corrupted by bounded noise in terms of $\ell_1$ or $\ell_{\infty}$, and propose an inversion scheme for generative models using linear programs layer-by-layer. 
The analysis for an assuredly stable inversion is restricted to cases where: 
(1) the weights of the network should be Gaussian \iid variables; 
(2) each layer should be expanded by a constant factor; 
and (3) the last activation function should be ReLU~\cite{nair2010rectified} or leaky-ReLU~\cite{maas2013rectifier}.
However, these assumptions often do not hold in practice. 
Aberdam~\etal~\cite{aberdam2020invert} relax the expansion assumption of~\cite{lei2019inverting}, and propose a method that relies on the expansion of the number of non-zero elements. 
They reformulate problem~\eqref{eqn:def} with $\ell_2$ to a layer-wise expression
\begin{equation}
\underset{\z}{\arg\min} \Bigl\|\y - \phi((\prod_{i=L}^{0} \W_i^{\hat{\mathcal{S}}_{i+1}})\z)\Bigr\|_2^2,
\end{equation}
where $\y=G(\z)+\mathbf{e}$, $\{\mathcal{S}_i\}_{i=1}^L$ are support sets of each layers, $\phi$ is an invertible activation function ReLU, and $\mathbf{W}_i^{\mathcal{S}}$ denotes the row-supported matrix according to the support set $\mathcal{S}$. 
Thus, the sparsity of all the intermediate feature vectors can be used to invert the model by solving sparse coding problems layer-by-layer.
This method does not rely on the distribution of the weights nor on the chosen activation function of the last layer. 
However, this approach can only be applied to invert very shallow networks.

To invert complex state-of-the-art GANs, Bau~\etal~\cite{bau2019seeing} propose to solve the easier problem of inverting the final layers $G_f$:
\begin{equation}
x = G_f(\rr^{*}),
\label{eqn:ganseeing}
\end{equation}
where $\rr^{*} = \underset{\rr}{\arg\min} \ell(G_f(\rr), \rr)$ and $\ell$ is a distance metric in the image feature space. 
They solve the inversion problem~\eqref{eqn:def} in a two-step hybrid GAN inversion framework: first constructing a neural network $E$ that approximately inverts the entire $G$ and computes an estimate $\z_0 = E(x)$, and subsequently solving an optimization problem to identify an intermediate representation $\rr^* \approx \rr_0 = g_n(\cdots(g_1(\z_0)))$ that generates a reconstructed image $G_f(\rr^*)$ to closely recover $x$.
For each layer $g_i \in \{g_1,...,g_n, G_f\}$, a small network $e_i$ is first trained to invert $g_i$. 
That is, defining $\rr_i = g_i(\rr_{i-1})$, the goal is to learn a network $e_i$ that approximates the computation $\rr_{i-1} \approx e_i(\rr_{i})$ and ensures the predictions of the network $e_i$ to well preserve the output of the layer $g_i$, \ie, $\mathbf{r_{i}} \approx g_i(e_i(\rr_{i}))$.
As such, $e_i$ is trained to minimize both left- and right-inversion losses:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{L}} & = \E_{\z}[||\rr_{i-1}- e(g_i(\rr_{i-1}))||_1], \\
\mathcal{L}_{\text{R}} & = \E_{\z}[||\rr_i - g_i(e(\rr_i))||_1],\\
e_i & = \underset{e}{\arg\min}\quad \mathcal{L}_{\text{L}} + \lambda_{\text{R}}\; \mathcal{L}_{\text{R}},
\end{aligned}
\end{equation}
where $||\cdot||_1$ denotes an $\mathcal{L}_1$ loss and $\lambda_{\text{R}}$ is set as 0.01 to emphasize the reconstruction of $\rr_{i-1}$.
To focus on training near the manifold of representations produced by the generator, this method uses sample $\z$ and layers $g_i$ to compute samples of $\rr_{i-1}$ and $\rr_i$, such that $\rr_{i-1} = g_{i-1}(\cdots g_1(\z))$.
Once all the layers are inverted, an inversion network for all of $G$ can be composed as
\begin{equation}
{E}^{*}= e_1(e_2(\cdots(e_n(e_f(x))))).
\end{equation}
The results can be further improved by fine-tuning the composed network $E^*$ to invert $G$ jointly as a whole and obtain the final result $E$.

\subsubsection{Non-interference}
\label{sec:non-inference}

When several attributes are involved, editing one may affect another since some semantics are not separated.
Non-interference GAN inversion aims to tackle multi-attribute image manipulation without interference.
This characteristic is also named multi-dimensional~\cite{nitzan2020harness} or conditional editing~\cite{shen2020interpreting} in other GAN inversion approaches.
For example, to edit multiple attributes, Shen~\etal~\cite{shen2020interpreting} formulate the inversion-based image manipulation as $x^{\prime}=G(\z^{*}+\alpha \n)$, where $\n \in \mathbb{R^d}$ is a unit normal vector indicating a hyperplane defined by two latent codes $\z_{1}$ and $\z_{1}$.
In this method, $k$ attributes $\{\z_{1}, \cdots, \z_{k}\}$ can form $m$ (where $m \leq k(k-1)/2$) corresponding hyperplanes $\{\n_{1}, \cdots, \n_{m}\}$. 
Non-interference manipulation of multi-attributes means that $\{\n_{1}, \cdots, \n_{m}\}$ should be orthogonal with each other. 
If this condition does not hold, some semantics will correlate with each other and $\n_{i}^{\top} \n_{j}$ can be used to measure the entanglement between the $i$-th and $j$-th semantics.
In particular, this method uses projection to orthogonalize different vectors. 
As shown in Figure~\ref{fig:projection}, given two hyperplanes with normal vectors $\n_{1}$ and $\n_{2}$, the goal is to find a projected direction $\n_{1}-(\n_{1}^{\top} \n_{2}) \n_{2}$, such that moving samples along this new direction can change ``attribute one'' without affecting ``attribute two''. 
For the case where multiple attributes are involved, they subtract the projection from the primal direction onto the plane that is constructed by all conditioned directions.
Other GAN inversion methods~\cite{guan2020faster,viazovetskyi2020distillation} based on the pretrained StyleGAN~\cite{karras2019style} or StyleGAN2~\cite{karras2020analyzing} models can also manipulate multiple attributes due to the stronger separability of $\mathcal{W}$ space than $\mathcal{Z}$ space.
However, as observed by recent methods~\cite{xia2020tedigan,liu2020style,wu2020stylespace}, some attributes remain entangled in the $\mathcal{W}$ space, leading to some unwanted changes when we manipulate a given image.
Instead of manipulating in the semantic $\mathcal{W}$ space, Liu~\etal~\cite{liu2020style} propose the $\mathcal{S}$ space (style space), where all facial attributes are almost linearly separable.  
The style code is formed by concatenating the output of all affine layers of StyleGAN2~\cite{karras2020analyzing} generator.
Experiments show that the $\mathcal{S}$ space can alleviate \textit{spatially entangled changes} and exert precise local modifications.
By intervening the style code $s \in \mathcal{S}$ directly, their method can manipulate different facial attributes along with various semantic directions without affecting others and can achieve fine-grained controls on local translations.

\fignoninference

\figrewrite

\subsubsection{Region-of-Interest}
\label{sec:local}


The region-of-interest property of GAN inversion allows editing some desired regions in a given image with user manipulation, which often involves additional tools to select the desired region, as shown in Figure~\ref{fig:local}.
For example, to locate and change a specific semantic relationship, Bau~\etal~\cite{bau2020rewriting} generalize a linear associative memory~\cite{kohonen1973matrix} to a nonlinear convolutional layer of a deep generator. 
Each layer within a model stores latent rules as a set of key-value relationships over hidden features.
They propose a constrained optimization process that can add or edit one specific rule within the associative memory while preserving the existing semantic relationships in the model.
As shown in Figure~\ref{fig:ganrewriting}, 
they provide a three-step rewriting process: copy, paste, and context to make model rewriting intuitive for a novice user. 
Abdal~\etal~\cite{abdal2019image2stylegan,abdal2020image2stylegan2} analyze the defective image embedding of StyleGAN trained on FFHQ~\cite{karras2019style}, \ie, the embedding of images with masked regions.
The experiments show that the StyleGAN embedding is quite robust to the defects in images, and the embeddings of different facial features are independent of each other~\cite{abdal2019image2stylegan}. 
Based on the observation, they develop a masked-based local manipulation method.
They find a plausible embedding for regions outside the mask and fill in reasonable semantic content in the masked pixels. 
The region-of-interest local editing results of their method can be found in Figure~\ref{fig:local} and Figure~\ref{fig:local_style_tranfer}.

\figroi

\subsubsection{Out-of-Distribution}
\label{sec:ood}


Some GAN inversion methods support inverting the images, especially real images in the wild, that are not generated by the same process of the training data.
We refer to this ability as out-of-distribution generalization~\cite{ren2019likelihood,hendrycks2016baseline,lee2018simple}.
This property is a prerequisite for GAN inversion methods to edit real images.
In~\cite{daras2020your}, Daras~\etal show that a local sparse layer (based on local context) can significantly help invert a GAN model than a dense layer. 
They demonstrate the generalization ability of the proposed method by manipulating an image of redshank searched via Google, which did not appear in the training process. 
Shen~\etal~\cite{shen2020interpreting} present the InterFaceGAN model for face editing, which can invert a target image back to a latent code and directly edit the inverted code.
They analyze how individual semantic properties are encoded in the latent space and show that a true-or-false facial attribute aligns with a linear subspace of the latent space. 
By simply modulating the latent code, this method is able to manipulate the gender, age, pose, and expression of given real facial images.
In~\cite{pan2020exploiting}, Pan~\etal propose the deep generative prior (DGP) to embed rich knowledge of natural images.
As a generic image prior, the DGP method can be used to restore the missing information of a degraded image by progressively reconstructing it under the discriminator metric.
Recently, Abdal~\etal~\cite{abdal2020styleflow} introduce the StyleFlow method to the conditional exploration of the StyleGAN latent space. 
The attribute-conditioned sampling and attribute-controlled editing of the StyleGAN are analyzed by the proposed conditional continuous normalizing flows. 
As demonstrated in Figure~\ref{fig:ood}, this method is able to handle extreme pose, asymmetrical expressions, and age diversity well compared to the concurrent techniques.
Zhu~\etal~\cite{zhu2020indomain} propose a domain-specific GAN inversion approach to recover the input image at both the pixel and semantic levels.
Although trained only with the FFHQ dataset, their model can generalize to not only real face images from multiple face datasets~\cite{chelnokova2014rewards, courset2018caucasian, yi2019apdrawinggan} but also paintings, caricatures, and black and white photos collected from the Internet.
Besides the image, recent methods also show out-of-distribution generalization ability for other modalities, \ie, sketch~\cite{richardson2020encoding} and text~\cite{xia2020tedigan}.

\figood