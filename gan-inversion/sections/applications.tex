\section{Applications}
\label{sec:applications}

Finding an accurate solution to the inversion problem allows us to further fine-tune the model weights to match the target image without losing downstream editing capabilities.
GAN inversion does not require task-specific dense-labeled datasets and can be applied to many tasks like image manipulation, image interpolation, image restoration, style tranfer, novel-view synthesis and even adversarial defense, as shown in Figure~\ref{fig:application}.

\subsection{Image Manipulation}
\label{sec:manipulation}

Given an image $x$, we want to edit its certain regions by manipulating its latent codes $\z$ and get the manipulated ${\mathbf{z^{\prime}}}$ of the target image ${x^{\prime}}$ by linearly transforming the latent representation from a trained GAN model $G$. 
This can be formulated in the framework of GAN inversion as the operation of adding a scaled difference vector:
\begin{equation}
x^{\prime}=G(\z^{*}+\alpha \n), 
\end{equation}
where $\n$ is the normal direction corresponding to a particular semantic in the latent space and $\alpha$ is the step for manipulation. 
In other words, if a latent code is moved towards a certain direction, the semantics contained in the output image should vary accordingly. 
For example, Xu~\etal~\cite{xu2021ghfeat} use a hierarchical encoder to obtain the sampled features, matching between the learned GH-Feat with the internal representation of the StyleGAN generator and leading to high-fidelity global and local editing results from multiple levels.
Voynov~\etal~\cite{voynov2020latent} determine the direction corresponding to the background removal or background blur gradually without changing the foreground.
In~\cite{shen2020interpreting} Shen~\etal~achieve single and multiple facial attribute manipulation by projecting and orthogonalizing different vectors.
Recently, Zhu~\etal~\cite{zhu2020indomain} perform semantic manipulation by either decreasing or increasing the semantic degree. 
Both methods~\cite{shen2020interpreting,zhu2020indomain} use a projection strategy to search for the semantic direction $\n$.

There are also some methods that can manipulate other information in images, \eg, geometry, texture, and color, than semantics. 
For example, some~\cite{abdal2020styleflow,abdal2019image2stylegan} can change pose rotation for face manipulation, and others~\cite{voynov2020latent} can manipulate geometry (\eg, zoom / shift / rotation), texture (\eg, background blur / add grass / sharpness), and color (\eg, lighting / saturation).

\figapp

\figcorrect

\subsection{Image Restoration}
\label{sec:restoration}

Suppose $\hat{x}$ is obtained via $\hat{x}=\phi(x)$ during acquisition, where $x$ is the distortion-free image and $\phi$ is a degradation transform.
Numerous image restoration tasks can be regarded as recovering $x$ given $\hat{x}$. 
A common practice is to learn a mapping from $\hat{x}$ to $x$, which often requires task-specific training for different $\phi$.
Alternatively, GAN inversion can employ statistics of $x$ stored in some prior, and search in the space of $x$ for an optimal $x$ that best matches $\hat{x}$ by viewing $\hat{x}$ as partial observations of $x$.
For example, Abdal~\etal~\cite{abdal2019image2stylegan,abdal2020image2stylegan2} observe that StyleGAN embedding is quite robust to the defects in images, \eg, masked regions.
Based on that observation, they propose an inversion-based image inpainting method by embedding the source defective image into the early layers of the $\mathcal{W}^+$ space to predict the missing content and into the later layers to maintain color consistency.
Pan~\etal~\cite{pan2020exploiting} claim that a fixed GAN generator is inevitably limited by the distribution of training data and its inversion cannot faithfully reconstruct unseen and complex images.
Thus, they present a relaxed and more practical reconstruction formulation for capturing statistics of natural images in a trained GAN model as the priors, \ie, the Deep Generative Prior (DGP).
To be specific, they reformulate \eqref{eqn:opt} such that it allows the generator parameters to be fine-tuned on the target image on-the-fly:
\begin{equation}
\theta^*,\, \z^* = \underset{\theta,\, \z}{\arg\min}\, \ell(\hat{x}, \phi(G(\z; \theta))).
\label{eqn:opt_dgp}
\end{equation}
Their method performs visually better than or comparable to state-of-the-art methods on colorization~\cite{larsson2016learning}, inpainting~\cite{ulyanov2018deep}, and super-resolution~\cite{shaham2019singan}.
While artifacts sometimes occur in synthesized face images by GAN models~\cite{karras2017progressive,karras2019style}, 
Shen~\etal~\cite{shen2020interpreting} show that the quality information encoded in the latent space can be used for restoration. 
The artifacts generated by PGGAN~\cite{karras2017progressive} can be corrected by moving the latent code towards the positive quality direction that defined by a separation hyperplane using a linear SVM~\cite{cortes1995support} (see Figure~\ref{fig:correction}). 
 
\subsection{Image Interpolation}
\label{sec:interpolation}

With GAN inversion, new results can be interpolated by morphing between corresponding latent vectors of given images.
Given a well-trained GAN generator $G$ and two target images $x_{A}$ and $x_{B}$, morphing between them could naturally be done by interpolating between their latent vectors $\z_{A}$ and $\z_{B}$. 
Typically, morphing between $x_{A}$ and $x_{B}$ can be obtained by applying linear interpolation~\cite{xia2020gaze,pan2020exploiting}: 
\begin{equation}
\z=\lambda \z_{A}+(1-\lambda) \z_{B}, \lambda \in(0,1).
\label{eqn:interp}
\end{equation}
Abdal~\etal~\cite{abdal2019image2stylegan} use linear interpolation operation on vectors by embedding the $\mathcal{W}$ space of StyleGAN into an extended latent space $\mathcal{W^{*}}$.
Similar operation can also be found in~\cite{nitzan2020harness}.
On the other hand, in DGP~\cite{pan2020exploiting}, reconstructing two target images $x_{A}$ and $x_{B}$ would result in two generators $G_{\theta_{A}}$ and $G_{\theta_{B}}$, and the corresponding latent vectors $\z_{A}$ and $\z_{B}$ since they also fine-tuned $G$. 
In this case, morphing between $x_{A}$ and $x_{B}$ can be achieved by linear interpolation to both the latent vectors and the generator parameters: 
\begin{equation}
\begin{aligned}
\z&=\lambda \z_{A}+(1-\lambda) \z_{B},\\ \theta&=\lambda \theta_{A}+(1-\lambda) \theta_{B}, \; \lambda \in(0,1),
\end{aligned}
\label{eqn:interp_dgp}
\end{equation}
and images can be generated with the new $\z$ and $\theta$.

\subsection{Style Transfer}
\label{sec:style_tranfer}

To transfer the style from one image to another or mix styles of two images, numerous methods based on GAN inversion have been proposed. 
Given two latent codes, style transfer can be defined as crossover operation~\cite{karras2019style,abdal2019image2stylegan}.
Abdal~\etal~\cite{abdal2019image2stylegan} introduce two style transfer formulations, one is between the embedded stylized image and other face images (\eg, a face photo and a face sketch), and the other is between embedded images from different classes (\eg, a face photo and a non-face painting). 
The latent codes of the embedded content image are preserved for the first 9 layers (corresponding to spatial resolution from $4^{2}$ to $64^{2}$), and the latent codes of the style image for the last 9 layers are overwritten (corresponding to spatial resolution from $64^{2}$ to $1024^{2}$).
In~\cite{abdal2020image2stylegan2}, Abdal~\etal~present a local style transfer method (see Algorithm~\ref{alg:local_style_tranfer} and Figure~\ref{fig:local_style_tranfer}).
An embedding algorithm as a gradient-based optimization is developed that iteratively updates an image starting from some initial latent code. 
The embedding is constructed with two spaces: the semantic $\w \in \mathcal{W^{+}}$ space and a noise $\n \in \mathcal{N}$ space encoding high frequency details.
Local style transfer modifies a region in the input image $x$ to transform it to the style defined by a style reference image $y$. 
The first step is to embed the image into the $\mathcal{W}^{+}$ space to obtain the code $\mathbf{w^*}$ and initializing noise code as $\mathbf{n_i}$.
The second step is to apply the Masked $\mathcal{W}^{+}$ Optimization $W_{l}$ along with the Masked Style Transfer $M_{st}$ using blurred mask $M_b$. 
Finally, they perform the Masked Noise Optimization $M k_{n}$ to output the final image. 
The $W_{l}$ function only optimizes $\w \in \mathcal{W}^{+}$ and is given by
\begin{equation}
\begin{aligned}
W_l(M_p, M_m, \mathbf{w_m}, \mathbf{w_i}, \mathbf{n_i},x) & = \underset{\mathbf{w_m}}{\arg\min}\, L_p(M_{p}, G(\w, \n), x) \\
& + \|M_{m} \odot (G(\w, \n) - x)\|,
\end{aligned}
\label{eqn:ml}
\end{equation}
where $L_p$ denotes the perceptual loss~\cite{johnson2016perceptual},
$\mathbf{w_i}$ and $\mathbf{n_i}$ are initial variable values, 
$\mathbf{w_m}$ is a mask for $\mathcal{W}^{+}$ space, $\odot$ denotes the Hadamard product, and $G$ is the StyleGAN generator.
$\mathbf{w_m}$ contains 1s for variables that should be updated and 0s for variables that should remain constant.
The $M_{st}$ function optimizes $\w$ to achieve a given target style defined by style image $y$, which is defined as
\begin{equation}
M_{st}(M_{s},\mathbf{w_i},\mathbf{n_i}, y) = \underset{\w}{\arg\min}\, L_s (M_s, G(\w, \n), y), 
\label{eqn:mst}
\end{equation}
where $L_s$ is the style loss~\cite{gatys2016image}.
The ${Mk}_{n}$ function optimizes $\n \in N_s$ only, leaving $\w$ constant: ${Mk}_{n}(M, \mathbf{w_i},\mathbf{n_i}, x, y) = \underset{\n}{\arg\min} \|M_m \odot (G(\w, \n) - x)\|
+ \|(1 - M_m) \odot (G(\w, \n) - y)\|$, where the noise space $N_s$ has dimensions $\{\R^{4\times4}, \cdots,\R^{1024\times1024}\}$.
Algorithm~\ref{alg:local_style_tranfer} shows the main steps of this method. 
They use an alternating optimization strategy,\ie, optimizing $\w$ while keeping $\n$ fixed and subsequently optimizing $\n$ while keeping $\w$ fixed.
Aside from local style transfer, this method can also be used for image inpainting and local edits using scribbles by applying different spatial masks $(M_s, M_p, M_m)$.

\algotransfer

\figtransfer

\subsection{Compressive Sensing}
\label{sec:compressing}
Typically, compressive sensing can be formulated as reconstructing an unknown target signal or image $x \in \R^{n}$ from observations $\y \in \R^{m}$ of the form $\y={A} x+\mathbf{e}$, where $A \in \R^{m \times n}$ is a measurement matrix, $\mathbf{e} \in \R^{m}$ represents stochastic noise.
Since the number of measurements is much smaller than the ambient dimension of the signal, \ie, $m \ll n$, the above inverse problem is ill-posed.
An alternative way for solution is to obtain an estimate of $\hat{x}$ as the solution to the constrained optimization problem:
\begin{equation}
\begin{aligned}
\hat{x} &= \arg\min~\ell(y; Ax),\\
&\text{s.t.}~~~x \in \mathcal{S},
\label{eqn:sc}
\end{aligned}   
\end{equation}
where $\ell$ is the loss function and $\mathcal{S} \subseteq \mathcal{R}^n$ acts as \emph{a priori}. 
To alleviate the ill-posed nature of the inversion problem~\eqref{eqn:sc} and make accurate recovery of $x^*$ possible, several assumptions are commonly made, \eg, the signal $x^* \in \mathcal{S}$ is sufficiently sparse and measurement matrix $A$ satisfies certain algebraic conditions, such as the restricted isometry property (RSP)~\cite{candes2006compressive} or the restricted
eigenvalue condition (REC)~\cite{donoho2006compressed}.

Applying GAN inversion to compressive sensing is to estimate the signal as $\hat{x}=G(\hat{\z})$, where $\hat{\z}$ is obtained by minimizing the non-convex cost function
\begin{equation}
\label{eqn:compressive}
f(\z)=\|\y-AG(\z)\|_{2}^{2}. 
\end{equation}
Bora~\etal~\cite{bora2017compressed} propose to solve~\eqref{eqn:compressive} using back-propagation and standard gradient-based optimization.
Hussein~\etal~\cite{hussein2019image} handle the limited representation capabilities of the generators by making them image-adaptive (IA) using internal learning at test-time.
Instead of recovering the latent signal $x$ as $\hat{x}=G(\hat{\z})$, where $G(\cdot)$ is a well-trained generator, they simultaneously optimize $\z$ and the parameters of the generator, denoted as $\theta$, by minimizing the cost function
\begin{equation}
f(\theta, \z)=\|y-AG_{\theta}(\z)\|_{2}^{2}.
\end{equation}
In~\cite{shah2018solving}, Shah~\etal~present a projected gradient descent (PGD)-based method to solve~\eqref{eqn:compressive}.
The first step of this approach is to update the gradient descent at the $t$-th iteration to obtain ${w_t}$: ${w_t} \leftarrow x_t + \eta A^{\top}(y-Ax_t)$ where $\eta$ denotes the learning rate, and the second step is to use $G$ to find the target image that matches the current estimate ${w_t}$ by defining the projection operator $\mathcal{P}_G$: $\mathcal{P}_G({w_t}) = G(\underset{\z}{\arg\min}\|{w_t} - G(\z)\|)$.
Based on~\cite{shah2018solving}, Raj~\etal~\cite{raj2019gan} replace the iterative scheme in the inner-loop with a learning-based approach, as it often performs better and does not fall into local optima.

\subsection{Other Tasks}
\subsubsection{Interactive Generation}
\label{sec:interactive}

GAN inversion methods can be applied to interactive generation, \ie, starting with strokes drawn by a user and generating natural images that best satisfy the user constraints. 
As shown in Figure~\ref{fig:interactive}, Zhu~\etal~\cite{zhu2016generative} show that users can use the brush tools to generate an image from scratch and then keep adding more scribbles to refine the result.
Bau~\etal~\cite{bau2019ganpaint} develop a tool (See https://ganpaint.io/demo/) that takes a natural image of a certain class, \eg, church or kitchen, and allows modifications with brushes to draw semantically meaningful units, such as trees or domes.
Abdal~\etal~\cite{abdal2020image2stylegan2} invert the StyleGAN to perform semantic local edits based on user scribbles. 
With this method, simple scribbles can be converted to photo-realistic edits 
by embedding into certain layers of StyleGAN.
This application is helpful to existing interactive image processing tasks such as sketch-to-image generation~\cite{xia2019sketch,ghosh2019isketchnfill,chenDeepFaceDrawing2020} and sketch based image retrieval~\cite{eitz2010sketch,dey2019doodle}, which usually require dense-labeled datasets.

\figinteractive

\subsubsection{Semantic Diffusion}
\label{sec:diffusion}
Semantic image diffusion is an image editing task, which inserts the target face to the context and makes them compatible, as illustrated in Figure~\ref{fig:diffusion}. 
It can be seen as a variant of image harmonization~\cite{cohen2006color,huang2018multimodal,tsai2017deep}.
Zhu~\etal~\cite{zhu2020indomain} use their in-domain inversion method for semantic diffusion, which keeps the characteristics of the target image (\eg, face identity) and adapts to the context information at the same time.
On the other hand, Xu~\etal~\cite{xu2021ghfeat} copy some patches (\eg, bed and window) onto a bedroom image and feed the stitched image into the proposed encoder for feature extraction. 
The extracted features are then visualized via the generator for image harmonization.

\subsubsection{Category Transfer}
\label{sec:category}
In Section~\ref{sec:restoration}, we demonstrate that DGP~\cite{pan2020exploiting}, a method proposed by Pan~\etal, can be used to restore images of different degradations.
Their method can also be used to transfer the object category of given images, by tweaking the class condition during the reconstruction. 
The lower right corner of  Figure~\ref{fig:application} shows an example that transfers the dog to various other categories without changing the pose, shape, or background.

\subsubsection{Adversarial Defense}
\label{sec:defense}
Adversarial attack methods~\cite{fan2020sparse,chen2020boosting,wu2020sapf,baluja2017adversarial} aim at fooling a CNN classifier by adding a certain perturbation $\Delta x$ to a target image $x$. 
In contrast, adversarial defense~\cite{samangouei2018defense,dhillon2018stochastic} aims at preventing the model from being fooled by attackers.
If considering the degradation transform of adversarial attack as $\phi(x)=x+\Delta x$, where $\Delta x$ is the perturbation generated by the attacker, we can use the inversion methods demonstrated in Section~\ref{sec:restoration} for adversarial defense.
For example, DGP~\cite{pan2020exploiting} directly reconstructs the adversarial image $\hat{x}$ and stops the reconstruction when the MSE loss reaches a certain threshold value.

\figdiffusion